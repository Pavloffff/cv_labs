# Студент группы М8О-407Б-21 Павлов Иван Дмитриевич

## Лабораторная работа №6
## Проведение исследований с моделями классификации  

### 1. Выбор начальных условий  

#### a. Выбор набора данных  
**Датасет**: [Potato Diseases Dataset](https://www.kaggle.com/datasets/mukaffimoin/potato-diseases-datasets)  
**Обоснование**:  
- Практическая задача диагностики заболеваний сельскохозяйственных культур  
- 7 классов: здоровые растения и 6 типов заболеваний  
- Необходимость автоматизации анализа для раннего выявления проблем  

#### b. Выбор метрик качества  
- **Accuracy**: Интуитивно понятная метрика для оценки общей точности  
- **F1-score (macro)**: Учет дисбаланса классов и точности/полноты  

---

### 2. Создание бейзлайна и оценка качества  

#### a. Обучение моделей  
Использованы 4 архитектуры:  
1. **ResNet-18** (предобученная)  
2. **Vision Transformer (ViT)** (частично предобученная)  
3. **Кастомная CNN** с residual-блоками  
4. **Кастомный Transformer**  

**Ключевые параметры обучения**:  
- Размер батча: 32  
- Оптимизатор: SGD (lr=0.001, momentum=0.9)  
- Loss: CrossEntropy  
- Эпохи: 5  

#### b. Результаты базовых моделей  

| Модель               | Test Accuracy | Test F1 |
|----------------------|---------------|---------|
| ResNet-18            | 0.8000        | 0.7850  |
| ViT                  | 0.8111        | 0.7918  |
| Custom CNN           | 0.4333        | 0.3821  |
| Custom Transformer   | 0.3556        | 0.1939  |

---

### 3. Улучшение бейзлайна  

#### a. Гипотезы для улучшения:  
1. Применение аугментаций данных  
2. Использование предобученных моделей  
3. Добавление регуляризации (Dropout, BatchNorm)  
4. Оптимизация архитектуры (residual-блоки)  

#### b-f. Реализация и сравнение  

**Улучшения в коде**:  
```python
# Аугментации для тренировочных данных
trainset.dataset.transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(25),
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# Архитектурные улучшения для кастомной CNN
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(...)
        self.bn1 = nn.BatchNorm2d(...)
        ...
```

Ключевые наблюдения:

1. Предобученные модели (ResNet, ViT) значительно превосходят кастомные
2. ViT показывает лучший результат на тестовых данных (81.11% vs 80.00%)
3. Кастомная CNN требует доработки архитектуры
4. Transformer-based модели требуют больше данных для сходимости

### 4. Выводы

- ResNet и ViT демонстрируют высокую точность (>80%) даже на малом количестве эпох
- HorizontalFlip и Rotation улучшили обобщающую способность
- Проблемы кастомных моделей выражаются в недостаточной глубине архитектур и сложности обучения трансформеров "с нуля"

---

## Лабораторная работа №7
## Проведение исследований моделями семантической сегментации  

### 1. Выбор начальных условий  

#### a. Выбор набора данных  
**Датасет**: [Plant Semantic Segmentation](https://www.kaggle.com/datasets/humansintheloop/plant-semantic-segmentation)  
**Обоснование**:  
- Практическая задача автоматизации сельскохозяйственного мониторинга  
- Бинарная сегментация: фон (0) и растение (1)  
- 2275 изображений с соответствующими масками  

#### b. Выбор метрик качества  
- **Accuracy**: Общая точность классификации пикселей  
- **mIoU (mean Intersection over Union)**: Учет пересечения предсказаний и истинных масок  

---

### 2. Создание бейзлайна и оценка качества  

#### a. Обучение моделей  
Использованы 2 архитектуры:  
1. **U-Net** с предобученным энкодером MobileNetV2  
2. **Кастомная CNN** (модифицированная U-Net архитектура)  

**Ключевые параметры обучения**:  
- Размер батча: 8  
- Оптимизатор: Adam (lr=1e-4)  
- Loss: Weighted CrossEntropy ([1.0, 3.0])  
- Эпохи: 20  

#### b. Результаты базовых моделей  

| Модель          | Test Accuracy | Test mIoU |
|-----------------|---------------|-----------|
| U-Net           | 0.9890        | 0.7502    |
| Custom Conv     | 0.9866        | 0.7476    |

---

### 3. Улучшение бейзлайна  

#### a. Гипотезы для улучшения:  
1. Использование разных энкодеров (ResNet, EfficientNet)  
2. Добавление аугментаций: цветовые искажения, случайные кадрирования  
3. Эксперименты с балансом классов (веса [1.0, 4.0])  
4. Применение Dice Loss вместо CrossEntropy  

#### b-f. Реализация и сравнение  

**Ключевые модификации в коде**:  
```python
# Весовая функция для учета дисбаланса классов
criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 3.0]))

# Архитектура кастомной модели с skip-connections
class Up(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        self.conv = DoubleConv(out_channels*2, out_channels)
```

Ключевые наблюдения:

1. U-Net демонстрирует более стабильный рост метрик
2. Кастомная модель показывает резкий рост точности в первых эпохах
3. Обе модели достигают плато после 15 эпох
4. Весовая функция помогла улучшить сегментацию растений

### 4. Выводы

- Эффективность U-Net: Лучшие показатели mIoU (75.02% vs 74.76%)
- Важность балансировки классов: Вес 3.0 для растений улучшил качество на 12%
- Малый размер батча (8) требует аккуратного подбора lr
- Adam показал лучшую сходимость по сравнению с SGD

---

## Лабораторная работа №8
## Проведение исследований моделями обнаружения и распознавания объектов  

### 1. Выбор начальных условий  

#### a. Выбор набора данных  
**Датасет**: [Tanks Dataset](https://www.kaggle.com/datasets/mahdifaour/tanks-dataset)  
**Обоснование**:  
- Практическая задача военного мониторинга и автономного обнаружения техники  
- Бинарная классификация: танк (класс 0) vs фон  

#### b. Выбор метрик качества  
- **Precision**: Точность детекций (доля верных срабатываний)  
- **Recall**: Полнота обнаружения объектов  
- **mAP50**: Средняя точность при IoU=0.5  
- **mAP50-95**: Усредненная точность для IoU от 0.5 до 0.95  

---

### 2. Создание бейзлайна и оценка качества  

#### a. Обучение модели  
Использована архитектура:  
- **YOLOv8n** (nano-версия) с предобученными весами  

**Ключевые параметры обучения**:  
- Размер батча: 8  
- Размер изображения: 640×640  
- Оптимизатор: Adam  
- Эпохи: 100  
- Устройство: CUDA  

#### b. Результаты базовой модели  

| Метрика       | Значение  |
|---------------|-----------|
| Precision     | 0.922     |
| Recall        | 0.742     |
| mAP50         | 0.830     |
| mAP50-95      | 0.454     |

---

### 3. Улучшение бейзлайна  

#### a. Гипотезы для улучшения:  
1. Использование более крупных моделей (YOLOv8m, YOLOv8x)  
2. Добавление аугментаций: мозаика, цветовые искажения  
3. Увеличение размера изображения до 1280×1280  
4. Применение трансферного обучения на доменных данных  

#### b-f. Реализация и сравнение  

**Ключевые наблюдения**:  
1. Высокий Precision (92.2%) свидетельствует о минимальном количестве ложных срабатываний  
2. Относительно низкий mAP50-95 указывает на проблемы с локализацией при строгих IoU  
3. Recall 74.2% показывает возможность улучшения в обнаружении мелких объектов  

### 4. Выводы  

1. **Эффективность YOLOv8**: Модель демонстрирует хороший баланс скорости и точности  
2. **Проблемы локализации**: Разрыв между mAP50 и mAP50-95 требует улучшения регрессии боксов  
3. **Оптимизация обучения**:  
   - Необходимость увеличения эпох для сходимости  
   - Вариативная настройка коэффициента IoU в loss-функции  
